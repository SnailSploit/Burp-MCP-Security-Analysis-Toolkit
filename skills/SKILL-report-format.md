# SKILL: Report Format & Generation

This skill defines the structure, format, and generation process for the final security assessment report.

## Report Purpose

The report must communicate:
1. What was tested (scope)
2. What was found (findings)
3. How severe it is (risk)
4. How to fix it (remediation)

Audience: Technical security teams, developers, and potentially management.

---

## Report Structure

```
output/report.md
├── Title & Metadata
├── Executive Summary
├── Scope & Methodology
├── Findings Summary Table
├── Detailed Findings (ordered by severity)
│   ├── Finding 1 (Critical)
│   ├── Finding 2 (Critical)
│   ├── Finding 3 (High)
│   └── ...
├── Informational Notes
├── Appendix: Testing Methodology
└── Appendix: Request Evidence Index
```

---

## Report Template

```markdown
# Security Assessment Report

**Target**: {target_domain}
**Assessment Date**: {date}
**Report Generated**: {datetime}
**Scope**: {scope_summary}

---

## Executive Summary

This assessment of **{target}** identified **{total_count}** security findings:

| Severity | Count |
|----------|-------|
| Critical | {critical_count} |
| High | {high_count} |
| Medium | {medium_count} |
| Low | {low_count} |
| Informational | {info_count} |

### Key Risks

{For each Critical/High finding, one sentence summary}

- **CRITICAL**: {finding_title} - {one_sentence_impact}
- **CRITICAL**: {finding_title} - {one_sentence_impact}
- **HIGH**: {finding_title} - {one_sentence_impact}

### Recommendations Priority

1. {Most critical remediation}
2. {Second priority}
3. {Third priority}

---

## Scope & Methodology

### Target Scope

| Attribute | Value |
|-----------|-------|
| Primary Target | {target} |
| Included Paths | {include_patterns} |
| Excluded Paths | {exclude_patterns} |
| Testing Period | {date_range} |

### Methodology

Testing was conducted using a phased approach:

1. **Traffic Analysis**: HTTP traffic captured via Burp Suite proxy
2. **Endpoint Triage**: Classification and prioritization of {endpoint_count} unique endpoints
3. **Targeted Analysis**: Focused testing for indicators: {enabled_indicators}
4. **Manual Verification**: All findings verified with evidence from actual traffic

### Authentication Contexts

| Context | Description |
|---------|-------------|
| {context_name} | {context_description} |

---

## Findings Summary

| ID | Title | Severity | Indicator | Endpoint |
|----|-------|----------|-----------|----------|
| F001 | {title} | Critical | {indicator} | {endpoint} |
| F002 | {title} | High | {indicator} | {endpoint} |
| ... | ... | ... | ... | ... |

---

## Detailed Findings

{For each finding, include full detail section}

---

## Informational Notes

{Non-vulnerability observations that may be useful}

- {Note about interesting but non-vulnerable behavior}
- {Note about hardening opportunities}
- {Note about best practices not followed but not exploitable}

---

## Appendix A: Testing Methodology Details

### Indicators Tested

{For each indicator, brief methodology summary}

#### IDOR Testing
- Identified {n} endpoints with object references
- Tested cross-user access patterns
- Verified authorization enforcement

#### Authentication Testing
- Analyzed {auth_type} authentication mechanism
- Tested token manipulation, expiration, bypass attempts
- Verified session management

{Continue for each indicator}

---

## Appendix B: Request Evidence Index

| Burp ID | Finding | Description |
|---------|---------|-------------|
| #1234 | F001 | IDOR attack request |
| #1235 | F001 | IDOR attack response |
| #1236 | F002 | Auth bypass attempt |
| ... | ... | ... |

---

*Report generated by Burp MCP Analysis Toolkit*
```

---

## Finding Template (Detailed)

Each finding in the Detailed Findings section:

```markdown
### F{id}: {title}

| Attribute | Value |
|-----------|-------|
| Severity | {severity} |
| CVSS Score | {cvss} (if calculable) |
| Indicator | {indicator_type} |
| Endpoint | {method} {path} |
| Status | Confirmed |

#### Description

{2-4 sentences explaining the vulnerability in clear terms}

The {endpoint} endpoint {what it does}. The application fails to {what's missing}.
This allows an attacker to {what they can do}.

#### Technical Details

**Vulnerable Component**: {specific component/parameter}
**Attack Vector**: {how to exploit}
**Authentication Required**: {Yes/No/Partial}
**User Interaction**: {Required/None}

#### Evidence

**Request** ({context}):
\`\`\`http
{Full HTTP request}
\`\`\`

**Response**:
\`\`\`http
{Relevant response portion}
\`\`\`

**Proof of Impact**:
{Explanation of why this proves the vulnerability}

**Burp Request ID**: #{id}

#### Impact

{What damage could result from exploitation}

- {Specific impact 1}
- {Specific impact 2}
- {Business impact if relevant}

#### Remediation

**Recommended Fix**:
{Primary remediation approach}

**Implementation**:
\`\`\`{language}
{Code example if applicable}
\`\`\`

**Additional Hardening**:
- {Additional recommendation 1}
- {Additional recommendation 2}

#### References

- {OWASP link if relevant}
- {CWE reference}
- {Vendor documentation}

---
```

---

## Severity Rating Guidelines

### Critical (CVSS 9.0-10.0)

```
Criteria (ANY of these):
- Unauthenticated remote code execution
- Full database access/extraction
- Authentication bypass to admin
- Mass PII exposure
- Cloud credential exposure (AWS keys, etc.)
- Crypto key exposure

Business Impact:
- Immediate, severe business damage
- Regulatory violation certain
- Requires emergency response
```

### High (CVSS 7.0-8.9)

```
Criteria (ANY of these):
- Authenticated code execution
- Limited SQLi (data extraction possible)
- IDOR with PII exposure
- Privilege escalation
- Stored XSS in sensitive context
- Account takeover possible

Business Impact:
- Significant damage likely
- Regulatory concerns
- Requires prompt remediation
```

### Medium (CVSS 4.0-6.9)

```
Criteria (ANY of these):
- Reflected XSS
- Information disclosure (non-PII)
- CSRF on sensitive actions
- IDOR with non-sensitive data
- Weak session management

Business Impact:
- Moderate damage possible
- Should be fixed in normal cycle
```

### Low (CVSS 0.1-3.9)

```
Criteria (ANY of these):
- Information disclosure (minimal)
- Clickjacking (limited impact)
- Verbose error messages
- Missing security headers
- Cookie without secure flag

Business Impact:
- Minor damage potential
- Fix when convenient
```

### Informational (CVSS 0.0)

```
Criteria:
- Best practice violations (not exploitable)
- Observations about architecture
- Potential future issues
- Hardening recommendations

Business Impact:
- No direct security impact
- Good to know
```

---

## Report Generation Process

### Step 1: Aggregate Findings

```
1. Read all files in output/findings/
2. Parse each finding
3. Validate required fields present
4. Assign unique finding IDs (F001, F002, ...)
```

### Step 2: Deduplicate

```
Check for duplicates:
- Same endpoint + same vulnerability type
- Same root cause across multiple endpoints

Merge strategy:
- Keep highest severity instance
- Note all affected endpoints
- Combine evidence
```

### Step 3: Sort by Severity

```
Sort order:
1. Critical (top)
2. High
3. Medium
4. Low
5. Informational (bottom)

Within same severity:
- Sort by indicator type
- Then by endpoint alphabetically
```

### Step 4: Generate Summary Statistics

```
Count findings by:
- Severity (for executive summary)
- Indicator type (for methodology section)
- Endpoint (for coverage)
```

### Step 5: Format Report

```
1. Apply template
2. Insert aggregated data
3. Format evidence code blocks
4. Validate markdown syntax
5. Output to output/report.md
```

---

## Evidence Formatting Rules

### HTTP Requests

```http
{METHOD} {PATH} HTTP/1.1
Host: {host}
{Header}: {value}
{Header}: {value}

{body if present}
```

### HTTP Responses

```http
HTTP/1.1 {status_code} {status_text}
{Header}: {value}

{relevant body portion - truncate if >50 lines}
```

### Truncation Rules

- Request body: Max 100 lines, truncate with `[... truncated ...]`
- Response body: Max 50 lines for report (full evidence in appendix)
- Sensitive data: Partially redact (show pattern, hide full value)
  - Token: `eyJ....[REDACTED]....xyz`
  - SSN: `***-**-6789`
  - Email: `u***@example.com`

### Code Examples

Always specify language for syntax highlighting:
- `http` for HTTP traffic
- `json` for JSON bodies
- `python`, `javascript`, etc. for code examples

---

## Quality Checklist

Before finalizing report:

```
Structure:
[ ] Executive summary present and accurate
[ ] All findings have required fields
[ ] Severity ratings justified
[ ] Evidence present for all findings

Accuracy:
[ ] No false positives included
[ ] Severity matches impact
[ ] Endpoint paths correct
[ ] Burp IDs valid

Clarity:
[ ] Descriptions understandable
[ ] Technical details accurate
[ ] Remediation actionable
[ ] No jargon without explanation

Format:
[ ] Markdown renders correctly
[ ] Code blocks formatted
[ ] Tables aligned
[ ] No broken links

Completeness:
[ ] All indicators analyzed
[ ] Scope accurately described
[ ] Methodology documented
[ ] Evidence indexed
```

---

## Output Location

Final report: `output/report.md`

Support files:
- `output/report.html` (optional, if HTML export requested)
- `output/report.json` (optional, for programmatic consumption)
- `output/evidence/` (full evidence files if needed)
